{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision with SageMaker's Image Classification Algorithm\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites and Preprocessing](#Prequisites-and-Preprocessing)\n",
    "3. [Fine-tuning the Image classification model](#Fine-tuning-the-Image-classification-model)\n",
    "4. [Training parameters](#Training-parameters)\n",
    "5. [Start the training](#Start-the-training)\n",
    "6. [Inference](#Inference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Image classification is a fundamental computer vision task that involves predicting the overall label/class of an image.  Modern computer vision techniques use neural net models for these kinds of tasks.  Although neural nets can achieve high accuracy for image classification, they can be quite difficult to use directly.  Amazon SageMaker's built-in image classification algorithm makes  such neural nets much easier to use.  Simply provide your dataset and specify a few parameters, and you can train and deploy a custom model.  \n",
    "\n",
    "This notebook is an end-to-end example of image classification to \"fine-tune\" a pretrained model. Fine-tuning, a former of \"transfer learning\" from one classification task to another, typically results in substantial time and cost savings compared to training from scratch.  We'll use SageMaker's built-in image classification algorithm in transfer learning mode to fine-tune a model previously trained on the well-known public ImageNet dataset.  This fine-tuned model will be used to classify a new dataset different from ImageNet. In particular, the pretrained model will be fine-tuned with the [Caltech-256 dataset](http://www.vision.caltech.edu/Image_Datasets/Caltech256/).  \n",
    "\n",
    "SageMaker's built-in image classification algorithm has an option for training from scratch as well as transfer learning.  Using the built-in algorithm's transfer learning mode frees you from having to modify the underlying neural net architecture, which otherwise would be necessary if you used the neural net directly from a code base.  There are many other conveniences provided by this built-in algorithm, such as the ability to automatically train faster on a cluster of many instances without requiring you to manage cluster setup and teardown.  \n",
    "\n",
    "To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prequisites and Preprocessing\n",
    "\n",
    "### Permissions and environment variables\n",
    "\n",
    "Here we set up the linkage and authentication for AWS services. There are three parts to this:\n",
    "\n",
    "* The IAM role used to give learning and hosting access to your data. This will be obtained from the role used to start the notebook.\n",
    "* The S3 bucket for training and model data.\n",
    "* The Amazon SageMaker image classification algoritm Docker image which you can use out of the box, without modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'ic-transfer-learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sess.boto_region_name\n",
    "image_name = sagemaker.image_uris.retrieve(region=boto3.Session().region_name, framework='image-classification')\n",
    "print(image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Fine-tuning the Image Classification model\n",
    "\n",
    "The Caltech 256 dataset consist of images from 256 categories plus a clutter category. It has a total of 30000 images, with a minimum of 80 images and a maximum of about 800 images per category. \n",
    "\n",
    "The image classification algorithm can take two types of input formats. The first is a [recordio format](https://mxnet.incubator.apache.org/faq/recordio.html), and the other is a [lst format](https://mxnet.incubator.apache.org/faq/recordio.html?highlight=im2rec). In this example, we will use the recordio format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import boto3\n",
    "\n",
    "def download(url):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "        \n",
    "def upload_to_s3(channel, file):\n",
    "    s3 = boto3.resource('s3')\n",
    "    data = open(file, \"rb\")\n",
    "    key = channel + '/' + file\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)\n",
    "\n",
    "\n",
    "# # caltech-256\n",
    "download('http://data.mxnet.io/data/caltech-256/caltech-256-60-train.rec')\n",
    "download('http://data.mxnet.io/data/caltech-256/caltech-256-60-val.rec')\n",
    "upload_to_s3('validation', 'caltech-256-60-val.rec')\n",
    "upload_to_s3('train', 'caltech-256-60-train.rec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll upload the data to Amazon S3 so it can be accessed by SageMaker for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Four channels: train, validation, train_lst, and validation_lst\n",
    "s3train = 's3://{}/{}/train/'.format(bucket, prefix)\n",
    "s3validation = 's3://{}/{}/validation/'.format(bucket, prefix)\n",
    "\n",
    "# upload the lst files to train and validation channels\n",
    "!aws s3 cp caltech-256-60-train.rec $s3train --quiet\n",
    "!aws s3 cp caltech-256-60-val.rec $s3validation --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the data available in S3 in the correct format for training, the next step is to actually train the model using the data. Before training the model, we need to setup the training parameters. The next section will explain the parameters in detail and dive into how to set up the training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now that we are done with the data setup, we are almost ready to train our image classfication model. To begin, let's  create a ``sageMaker.estimator.Estimator`` object. This Estimator will launch the training job.\n",
    "\n",
    "### Training parameters\n",
    "\n",
    "There are two kinds of parameters to set for training. The first kind is the parameters for the training job itself, such as amount and type of hardware to use, and S3 location. For this example, these include:\n",
    "\n",
    "* **Instance count**: This is the number of instances on which to run the training. When the number of instances is greater than one, then the image classification algorithm will run in a distributed cluster automatically without requiring you to manage cluster setup. \n",
    "* **Instance type**: This indicates the type of machine on which to run the training. Typically, we use GPU instances for computer vision models such as this one.\n",
    "* **Output path**: This the S3 folder in which the training output will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "\n",
    "ic = sagemaker.estimator.Estimator(\n",
    "                                    image_uri=image_name,\n",
    "                                    role=role,\n",
    "                                    instance_count=1,\n",
    "                                    instance_type='ml.p3.8xlarge',\n",
    "                                    volume_size = 50,\n",
    "                                    sagemaker_session=sess,\n",
    "                                    output_path=s3_output_location )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the above set of training job parameters, the second set of parameters are hyperparameters that are specific to the algorithm. These include:\n",
    "\n",
    "* **num_layers**: The number of layers (depth) for the network. We use 18 in this example, but other values such as 50, 152 can be used to achieve greater accuracy at the cost of longer training time.\n",
    "* **use_pretrained_model**: Set to 1 to use a pretrained model for transfer learning.\n",
    "* **image_shape**: The input image dimensions,'num_channels, height, width', for the network. It should be no larger than the actual image size. The number of channels should be same as the actual image.\n",
    "* **num_classes**: This is the number of output classes for the new dataset. Imagenet has 1000  classes, but the number of output classes for our pretrained network can be changed with fine-tuning. For this Caltech dataset, we use 257 because it has 256 object categories + 1 clutter class.\n",
    "* **num_training_samples**: This is the total number of training samples. It is set to 15240 for the Caltech dataset due to the current split between training and validation data.\n",
    "* **mini_batch_size**: The number of training samples used for each mini batch. In distributed training for multiple training instances (we just use one here), the number of training samples used per batch would be N * mini_batch_size, where N is the number of hosts on which training is run.\n",
    "* **epochs**: Number of training epochs, i.e. passes over the complete training data.\n",
    "* **learning_rate**: Learning rate for training.\n",
    "* **precision_dtype**: Training datatype precision (default: float32). If set to 'float16', the training will be done in mixed_precision mode and will be faster than float32 mode, at the cost of slightly less accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "ic.set_hyperparameters(\n",
    "                         num_layers=18,\n",
    "                         use_pretrained_model=1,\n",
    "                         image_shape = \"3,224,224\",\n",
    "                         num_classes=257,\n",
    "                         num_training_samples=15420,\n",
    "                         mini_batch_size=128,\n",
    "                         epochs=2,\n",
    "                         learning_rate=0.01,\n",
    "                         precision_dtype='float32' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data specification\n",
    "\n",
    "The next step is to set the data type and channels used for training.  The channel definitions inform SageMaker about where to find both the training and validation datasets in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.inputs.TrainingInput(s3_data=s3train, content_type='application/x-recordio')\n",
    "validation_data = sagemaker.inputs.TrainingInput(s3_data=s3validation, content_type='application/x-recordio')\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the training job\n",
    "\n",
    "Now we can start the training job by calling the `fit` method of the Estimator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "***\n",
    "\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference, i.e. get predictions from the model. For this example, that means predicting the Caltech-256 class of a given image. To deploy the trained model, we simply use the `deploy` method of the Estimator.  This will create a SageMaker endpoint that can return predictions in real time, for example for use with a consumer-facing app that must have low latency responses to user requests.  SageMaker also can perform offline batch, asynchronous inference with its Batch Transform feature.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_classifier = ic.deploy(initial_instance_count = 1,\n",
    "                          instance_type = 'ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O test.jpg https://raw.githubusercontent.com/awslabs/amazon-sagemaker-workshop/master/images/clawfoot_bathtub.jpg\n",
    "file_name = 'test.jpg'\n",
    "# test image\n",
    "from IPython.display import Image\n",
    "Image(file_name)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Let's now use the SageMaker endpoint hosting the trained model to predict the Caltech-256 class of the test image. The model outputs class probabilities.  Typically, one selects the class with the maximum probability as the final predicted class output.\n",
    "\n",
    "**Note:** Although the output class detected by the network is likely to predict the correct class (bathtub), it is not guaranteed to be accurate as model training is a stochastic process. To limit the training time and related cost, we have trained the model only for a couple of epochs. If the model is trained for more epochs (say 20), the output class will be more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sagemaker.serializers import IdentitySerializer\n",
    "\n",
    "with open(file_name, 'rb') as f:\n",
    "    payload = f.read()\n",
    "    payload = bytearray(payload)\n",
    "    \n",
    "ic_classifier.serializer = IdentitySerializer(content_type='application/x-image')\n",
    "\n",
    "result = json.loads(ic_classifier.predict(payload))\n",
    "# output the probabilities for all classes, then find the class with maximum probability and print its index\n",
    "index = np.argmax(result)\n",
    "object_categories = ['ak47', 'american-flag', 'backpack', 'baseball-bat', 'baseball-glove', 'basketball-hoop', 'bat', 'bathtub', 'bear', 'beer-mug', 'billiards', 'binoculars', 'birdbath', 'blimp', 'bonsai-101', 'boom-box', 'bowling-ball', 'bowling-pin', 'boxing-glove', 'brain-101', 'breadmaker', 'buddha-101', 'bulldozer', 'butterfly', 'cactus', 'cake', 'calculator', 'camel', 'cannon', 'canoe', 'car-tire', 'cartman', 'cd', 'centipede', 'cereal-box', 'chandelier-101', 'chess-board', 'chimp', 'chopsticks', 'cockroach', 'coffee-mug', 'coffin', 'coin', 'comet', 'computer-keyboard', 'computer-monitor', 'computer-mouse', 'conch', 'cormorant', 'covered-wagon', 'cowboy-hat', 'crab-101', 'desk-globe', 'diamond-ring', 'dice', 'dog', 'dolphin-101', 'doorknob', 'drinking-straw', 'duck', 'dumb-bell', 'eiffel-tower', 'electric-guitar-101', 'elephant-101', 'elk', 'ewer-101', 'eyeglasses', 'fern', 'fighter-jet', 'fire-extinguisher', 'fire-hydrant', 'fire-truck', 'fireworks', 'flashlight', 'floppy-disk', 'football-helmet', 'french-horn', 'fried-egg', 'frisbee', 'frog', 'frying-pan', 'galaxy', 'gas-pump', 'giraffe', 'goat', 'golden-gate-bridge', 'goldfish', 'golf-ball', 'goose', 'gorilla', 'grand-piano-101', 'grapes', 'grasshopper', 'guitar-pick', 'hamburger', 'hammock', 'harmonica', 'harp', 'harpsichord', 'hawksbill-101', 'head-phones', 'helicopter-101', 'hibiscus', 'homer-simpson', 'horse', 'horseshoe-crab', 'hot-air-balloon', 'hot-dog', 'hot-tub', 'hourglass', 'house-fly', 'human-skeleton', 'hummingbird', 'ibis-101', 'ice-cream-cone', 'iguana', 'ipod', 'iris', 'jesus-christ', 'joy-stick', 'kangaroo-101', 'kayak', 'ketch-101', 'killer-whale', 'knife', 'ladder', 'laptop-101', 'lathe', 'leopards-101', 'license-plate', 'lightbulb', 'light-house', 'lightning', 'llama-101', 'mailbox', 'mandolin', 'mars', 'mattress', 'megaphone', 'menorah-101', 'microscope', 'microwave', 'minaret', 'minotaur', 'motorbikes-101', 'mountain-bike', 'mushroom', 'mussels', 'necktie', 'octopus', 'ostrich', 'owl', 'palm-pilot', 'palm-tree', 'paperclip', 'paper-shredder', 'pci-card', 'penguin', 'people', 'pez-dispenser', 'photocopier', 'picnic-table', 'playing-card', 'porcupine', 'pram', 'praying-mantis', 'pyramid', 'raccoon', 'radio-telescope', 'rainbow', 'refrigerator', 'revolver-101', 'rifle', 'rotary-phone', 'roulette-wheel', 'saddle', 'saturn', 'school-bus', 'scorpion-101', 'screwdriver', 'segway', 'self-propelled-lawn-mower', 'sextant', 'sheet-music', 'skateboard', 'skunk', 'skyscraper', 'smokestack', 'snail', 'snake', 'sneaker', 'snowmobile', 'soccer-ball', 'socks', 'soda-can', 'spaghetti', 'speed-boat', 'spider', 'spoon', 'stained-glass', 'starfish-101', 'steering-wheel', 'stirrups', 'sunflower-101', 'superman', 'sushi', 'swan', 'swiss-army-knife', 'sword', 'syringe', 'tambourine', 'teapot', 'teddy-bear', 'teepee', 'telephone-box', 'tennis-ball', 'tennis-court', 'tennis-racket', 'theodolite', 'toaster', 'tomato', 'tombstone', 'top-hat', 'touring-bike', 'tower-pisa', 'traffic-light', 'treadmill', 'triceratops', 'tricycle', 'trilobite-101', 'tripod', 't-shirt', 'tuning-fork', 'tweezer', 'umbrella-101', 'unicorn', 'vcr', 'video-projector', 'washing-machine', 'watch-101', 'waterfall', 'watermelon', 'welding-mask', 'wheelbarrow', 'windmill', 'wine-bottle', 'xylophone', 'yarmulke', 'yo-yo', 'zebra', 'airplanes-101', 'car-side-101', 'faces-easy-101', 'greyhound', 'tennis-shoes', 'toad', 'clutter']\n",
    "print(\"Result: label - \" + object_categories[index] + \", probability - \" + str(result[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "\n",
    "When we're done with the endpoint, we can just delete it and the backing instance will be released.  Run the following cell to delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(ic_classifier.endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
